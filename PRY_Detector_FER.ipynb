{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mioti.png\" >\n",
    "\n",
    "\n",
    "# Proyecto Reconocimiento Facial: Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/emociones.png\" style=\"width: 800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este notebook vamos a probar la combinación de los diferentes modelos generados anteriormente. El objetivo es que en una grabación en tiempo real, el sistema sea capaz de poner el nombre de la persona identificada, si esta ha sido registrada, en caso contrario será 'desconocido' y la emoción de esta persona, esto se realizará para todos los rostros identificados, sean conocidos por el modelo o no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para poner en práctica nuestro reconocedor utilizaremos estas librerías:\n",
    "    * cv2: para trabajar con las imágenes de video y aplicar el reconocedor de personas\n",
    "    * os: para trabajar con directorios\n",
    "    * numpy: para realizar operaciones matemáticas\n",
    "    * tensorflow: para definir modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-10T10:43:38.615577Z",
     "start_time": "2022-01-10T10:43:38.605579Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:25:52.291937Z",
     "start_time": "2022-01-18T16:25:25.606825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vamos a cargar nuestros modelos con los ficheros json y h5 generados durante el entrenamiento de la red de emociones para nuestro modelo de emociones y el fichero xlm para cargar nuestro modelo de reconocimiento de personas\n",
    "\n",
    "* Después en nuestra función face_detector vamos a incluir las predicciones realizadas por nuestros modelos\n",
    "\n",
    "* Los modelos siempre van a devolver una clase en cada predicción, en el caso de las emociones esto no genera ningún problema ya que queremos identificar el estado de animo, sin embargo en el caso de las personas no siempre vamos a conocer a quién estamos midiendo, para evitar el caso en que a una persona desconocida se le asigne el nombre de alguien registrado vamos a establecer un valor máximo a un umbral y cualquier valor que se encuentre por encima de este hará que esa persona sea clasificada como 'desconocido', en nuestro caso hemos establecido ese umbral por debajo de 10000 tras realizar diferentes pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:26:46.645086Z",
     "start_time": "2022-01-18T16:26:46.550078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Carga del modelo Haarcascade.\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:26:52.184994Z",
     "start_time": "2022-01-18T16:26:52.165998Z"
    }
   },
   "outputs": [],
   "source": [
    "#Carga del modelo de detección de emociones.\n",
    "\n",
    "file_model=\"./model/vgg-face-model-v4.json\"\n",
    "file_weights=\"./model/vgg-face-v4.h5\"\n",
    "class_labels=['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:26:58.987004Z",
     "start_time": "2022-01-18T16:26:57.812026Z"
    }
   },
   "outputs": [],
   "source": [
    "json_file = open(file_model, 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "reconocedor_emociones = tf.keras.models.model_from_json(model_json)\n",
    "reconocedor_emociones.load_weights(file_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:28:11.441722Z",
     "start_time": "2022-01-18T16:27:23.759669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Carga del modelo detección de personas.\n",
    "reconocedor_facial=cv2.face.EigenFaceRecognizer_create()\n",
    "reconocedor_facial.read('./model/modelo_reconocimiento_caras.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:28:17.336922Z",
     "start_time": "2022-01-18T16:28:17.315929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Javier', 'Juan Pedro', 'Ricardo']\n"
     ]
    }
   ],
   "source": [
    "data_path='./reconocimiento'\n",
    "lista_gente=os.listdir(data_path) \n",
    "lista_gente=['Javier', 'Juan Pedro', 'Ricardo']\n",
    "print(lista_gente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:28:52.164661Z",
     "start_time": "2022-01-18T16:28:52.145653Z"
    }
   },
   "outputs": [],
   "source": [
    "#Función que visualiza los textos en las imagenes.\n",
    "def draw_text_with_backgroud(img, text, x, y, font_scale, thickness=1, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            background=(175,50,200), foreground=(255,255,255), box_coords_1=(-5,5), box_coords_2=(5,-5)):\n",
    "    (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0]\n",
    "    box_coords = ((x+box_coords_1[0], y+box_coords_1[1]), (x + text_width + box_coords_2[0], y - text_height + box_coords_2[1]))\n",
    "    cv2.rectangle(img, box_coords[0], box_coords[1], background, cv2.FILLED)\n",
    "    cv2.putText(img, text, (x, y), font, fontScale=font_scale, color=foreground, thickness=thickness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:28:57.727339Z",
     "start_time": "2022-01-18T16:28:57.698330Z"
    }
   },
   "outputs": [],
   "source": [
    "#Función que orquesta, apoyandose en los tres modelos anteriores:\n",
    "#     Captura del rostro - face_classifier.\n",
    "#     Reconocimiento de personas - reconocedor_facial\n",
    "#     Reconocimiento de emociones - reconocedor_emociones\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.1, 4)\n",
    "    if len(faces) == 0:\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for idx,face in enumerate(faces):\n",
    "        x,y,w,h = face\n",
    "    \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(175,50,200),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        #ROI para el modelo reconocedor de emociones.\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_CUBIC)\n",
    "        #ROI para el modelo reconocedor de personas.\n",
    "        roi_gray_face = cv2.resize(roi_gray, (150, 150), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        #Procesamos el roi\n",
    "        if np.sum([roi_gray]) != 0.0:\n",
    "            roi = roi_gray.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "            \n",
    "            #Predicción de persona\n",
    "            result=reconocedor_facial.predict(roi_gray_face)\n",
    "            \n",
    "            #Predicción de emoción\n",
    "            preds = reconocedor_emociones.predict(roi)[0]\n",
    "            \n",
    "            if result[1]<11000:\n",
    "                label = f'Persona {lista_gente[result[0]]} EMOCION {class_labels[preds.argmax()]}'  \n",
    "            else:\n",
    "                label = f'Persona Desconocido EMOCION {class_labels[preds.argmax()]}'\n",
    "\n",
    "            draw_text_with_backgroud(img, label, x + 5, y, font_scale=0.5)\n",
    "            if (idx==1):\n",
    "                draw_text_with_backgroud(img, label, x + 5, y, font_scale=0.5, background=(15,150,200), foreground=(255,255,255), box_coords_1=(-7,7), box_coords_2=(7,-7))\n",
    "        else:\n",
    "            draw_text_with_backgroud(img, \"No Face Found\", x + 5, y, font_scale=0.5)\n",
    "   \n",
    "            \n",
    "    return (x,w,y,h), roi_gray, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución de ReconocIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo irá identificando a las personas y sus emociones captadas durante la reproducción de un video, pero sin guardar esos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:30:24.523831Z",
     "start_time": "2022-01-18T16:29:25.771792Z"
    }
   },
   "outputs": [],
   "source": [
    "#Camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "# SIN GUARDAR\n",
    "while True:\n",
    "\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "\n",
    "    \n",
    "    res = cv2.resize(image, dsize=(400,400),interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.namedWindow(\"ReconocIAs\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"ReconocIAs\", res)\n",
    "    \n",
    "#     cv2.resizeWindow('Reconocedor de Emociones', 900, 900)\n",
    "    if cv2.waitKey(1) == 13: #13 es la tecla Entrer\\is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiempo Real con Registro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo va a identificar a las personas que aparezcan en el vídeo, que será grabado con los resultados obtenidos en formato mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:32:04.829731Z",
     "start_time": "2022-01-18T16:31:35.945567Z"
    }
   },
   "outputs": [],
   "source": [
    "#Entrada\n",
    "#File camera o video.avi mp4\n",
    "\n",
    "b_Camera=True\n",
    "b_Avi_Format=True\n",
    "if (b_Camera):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture('./images/videoEntrada.avi')\n",
    "\n",
    "#SALIDA FORMATO AVI\n",
    "if (b_Avi_Format):\n",
    "    salida = cv2.VideoWriter('./images/videoSalida.avi',cv2.VideoWriter_fourcc(*'XVID'),6.0,(640,480))\n",
    "#SALIDA FORMATO MP4\n",
    "else:\n",
    "    salida = cv2.VideoWriter('./images/videoSalida.mp4',cv2.VideoWriter_fourcc(*'mp4v'),20.0,(640,480)) \n",
    "    \n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    cv2.namedWindow(\"ReconocIAs\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('\"ReconocIAs\"', image)\n",
    "    salida.write(image)\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 es la tecla Entrer\\is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "salida.release()\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen Estática con Registro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo recoge una fotografía de un directorio dado la analiza y guarda los resultados en el mismo directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T16:36:00.296640Z",
     "start_time": "2022-01-18T16:35:58.795640Z"
    }
   },
   "outputs": [],
   "source": [
    "#File jpg\n",
    "cap = cv2.imread('./images/imagenEntrada.jpg')\n",
    "processed=False\n",
    "while not processed:\n",
    "\n",
    "    frame = cap\n",
    "    rect, face, image = face_detector(frame)\n",
    "  \n",
    "    cv2.imwrite('./images/imagenSalida.jpg',image)\n",
    "    processed=True\n",
    "    if cv2.waitKey(1) == 13: #13 es la tecla Entrer\\is the Enter Key\n",
    "        break\n",
    "              \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de video sin procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graba un video en formato avi y lo guarda en un directorio sin realizar ningún tipo de reconocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-10T11:14:06.747074Z",
     "start_time": "2022-01-10T11:13:50.789479Z"
    }
   },
   "outputs": [],
   "source": [
    "captura = cv2.VideoCapture(0)\n",
    "salida = cv2.VideoWriter('./images/videoSalida.avi',cv2.VideoWriter_fourcc(*'XVID'),20.0,(640,480))\n",
    "while (captura.isOpened()):\n",
    "  ret, imagen = captura.read()\n",
    "  if ret == True:\n",
    "    cv2.imshow('video', imagen)\n",
    "    salida.write(imagen)\n",
    "    if cv2.waitKey(1) == 13: #13 es la tecla Entrer\\is the Enter Key\n",
    "      break\n",
    "  else: break\n",
    "captura.release()\n",
    "salida.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-10T14:00:27.467655Z",
     "start_time": "2022-01-10T14:00:27.437659Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "captura.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T10:25:31.417772Z",
     "start_time": "2021-12-17T10:25:31.375771Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.85px",
    "left": "1127px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
